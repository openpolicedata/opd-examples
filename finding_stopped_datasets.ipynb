{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding datasets that have stopped being updated\n",
    "This notebook shows how to find datasets that have stopped being updated. \n",
    "It looks across all datasets and finds the datasets that have been stopped and then does analysis with the table types to show which have the highest percentage of being stopped. The dataset with the highest stopped percentage is further analyzed to for year and regional trends.\n",
    "\n",
    "## Visualizations Included\n",
    "1. Years of Data Available by Source, State, and Table Type\n",
    "2. Time-series analysis of stopped datasets by year and table type\n",
    "3. Cumulative and percentage-based trends in dataset discontinuation\n",
    "4. Regional variations in stopped datasets for specific table types\n",
    "\n",
    "## Configurable Analysis Parameters\n",
    "Parameters that can be adjusted to customize the analysis:\n",
    "\n",
    "- **`remove_california_stops_data`**: Excludes California stops data (default: `True`)\n",
    "- **`clean_ois`**: Validates officer-involved shooting datasets using external data (default: `True`)\n",
    "- **`create_debug_file`**: Generates debug files for removed datasets (default: `False`) \n",
    "- **`analysis_year_min`/`analysis_year_max`**: Sets the year range for analysis (default: 1960-2025)\n",
    "- **`up_to_date_dataset_year_max`**: Defines threshold for \"current\" datasets (default: 2024)\n",
    "- **`minimum_tabletype_counts_to_show`**: Sets minimum dataset count for table type inclusion (default: 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openpolicedata as opd\n",
    "import pandas as pd\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(f\"Using openpolicedata version {opd.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are the parameters for the analysis that can be adjusted\n",
    "\n",
    "# Flag to remove California stops data. Data collection may have stopped when data collection started being handled at the state-level\n",
    "remove_california_stops_data = True\n",
    "\n",
    "# Flag to validate that data collection for officer-involved shootings (OIS) has actually stopped by looking for more \n",
    "# recent OIS in the Mapping Police Violence data. Since OIS are rare, this helps distinguish cases where OIS just have\n",
    "# not occurred recently\n",
    "clean_ois = True\n",
    "\n",
    "# Flag to create a debug file with removed datasets\n",
    "create_debug_file = False\n",
    "\n",
    "# The below parameters are used to configure the final graphs\n",
    "\n",
    "# These are the minimum and maximum years for what the final graphs will show\n",
    "analysis_year_min = 1960\n",
    "analysis_year_max = 2025\n",
    "up_to_date_dataset_year_max = analysis_year_max - 1  # Datasets whose most recent data is before this year are categorized as having stopped being updated\n",
    "\n",
    "# This is the minimum number of counts needed to show a table type in the final graphs\n",
    "minimum_tabletype_counts_to_show = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions go in this cell\n",
    "def remove_rows(remove_condition, all_datasets, debug_filename=None):\n",
    "    \n",
    "    starting_all_datasets_count = all_datasets.shape[0]\n",
    "    \n",
    "    remove_datasets = all_datasets[remove_condition]\n",
    "        \n",
    "    # # Drop the identified rows\n",
    "    all_datasets.drop(remove_datasets.index, inplace=True)\n",
    "        \n",
    "    # Verify the number of datasets removed\n",
    "    assert len(remove_datasets) == (starting_all_datasets_count - all_datasets.shape[0]), \\\n",
    "        \"Mismatch in the number of datasets removed\"\n",
    "        \n",
    "    # Save removed datasets if a debug filename is provided\n",
    "    if create_debug_file:\n",
    "        remove_datasets.to_csv(debug_filename, index=True)\n",
    "    \n",
    "    return all_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_ois_datasets(df):\n",
    "    keep = pd.Series(True, index=df.index)\n",
    "\n",
    "    # Get Mapping Police Violence data. MPV is a pretty thorough tracking of police killings.\n",
    "    # It does not include OIS but generally should be a good indicator whether an OIS dataset\n",
    "    # has seized to be updated or there just haven't been any OIS\n",
    "    src = opd.Source(\"Mapping Police Violence\")\n",
    "    t = src.load(\"OFFICER-INVOLVED SHOOTINGS\", \"MULTIPLE\")\n",
    "    t.standardize()\n",
    "\n",
    "    df_mpv = t.table\n",
    "\n",
    "    # Only keep killings via Gunshot. Others might not be included in OIS data\n",
    "    df_mpv = df_mpv[df_mpv['Cause of death']=='Gunshot']\n",
    "    \n",
    "    for i in df[df['TableType']=='OFFICER-INVOLVED SHOOTINGS'].index:\n",
    "        if df.loc[i,'MaxYear'] >= up_to_date_dataset_year_max:\n",
    "            continue  # This dataset will be classified as up-to-date\n",
    "\n",
    "        state_abbrev = opd.defs.states[df.loc[i, 'State']]\n",
    "        df_mpv_coarse = df_mpv[df_mpv['State']==state_abbrev]  # Filter MPV for OPD state\n",
    "        # Conservative filtering for OPD agency (only needs to contain partial agency name, not full one)\n",
    "        df_mpv_coarse = df_mpv_coarse[df_mpv_coarse['AGENCY'].str.lower().str.contains(df.loc[i, 'SourceName'].lower())]\n",
    "\n",
    "        num_coarse = len(df_mpv_coarse)\n",
    "\n",
    "        # Clean up MPV agency names\n",
    "        agencies = df_mpv_coarse['AGENCY'].apply(lambda x: re.sub(rf'\\s\\(?{state_abbrev}\\)?\\s', ' ', x, flags=re.IGNORECASE)).str.lower()\n",
    "        # Filter MPV data for full agency name\n",
    "        df_mpv_cur = df_mpv_coarse[agencies.str.contains(df.loc[i, 'AgencyFull'].lower())]\n",
    "\n",
    "        if len(df_mpv_cur)==0:\n",
    "            if num_coarse>0:\n",
    "                raise ValueError(f\"Unable to find any OIS in the MPV database for {df.loc[i, 'SourceName']}, {state_abbrev}\")\n",
    "        elif df_mpv_cur['DATE'].max().year>df.loc[i,'MaxYear']:\n",
    "                continue  # MPV agrees that this dataset has stopped being updated\n",
    "\n",
    "        # MPV data suggests that OPD dataset may be current despite most recent OIS being old\n",
    "        keep.loc[i] = False\n",
    "        print(f\"{i}: Removing {df.loc[i, 'SourceName']}, {df.loc[i, 'State']}\")\n",
    "\n",
    "    df = remove_rows(remove_condition=~keep,\n",
    "                     all_datasets=df,\n",
    "                     debug_filename='removed_ois_datasets.csv')\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell filters out datasets that are not of interest to the analysis\n",
    "\n",
    "all_datasets_original = opd.datasets.query()\n",
    "# all_datasets is the filtered dataset with only the rows we are interested in\n",
    "all_datasets = all_datasets_original.copy()\n",
    "\n",
    "# Remove datasets where the data is not supplied by the agency itself (such as MuckRock and Stanford)\n",
    "# supplying_entity is only filled out in this case. Ignore ones where OPD is hosting the data\n",
    "# OPD only hosts datasets that were released by the agency but the agency removed the data.\n",
    "all_datasets=remove_rows(remove_condition=all_datasets['supplying_entity'].apply(lambda x: not (pd.isnull(x) or 'OpenPoliceData' in x)),\n",
    "                        all_datasets=all_datasets,\n",
    "                        debug_filename=\"removed_bad_supplying_entity_datasets.csv\")\n",
    "\n",
    "\n",
    "if remove_california_stops_data:\n",
    "    # Remove all datasets whose State column is equal to California and are of type STOPS\n",
    "    all_datasets=remove_rows(remove_condition=(all_datasets['State'].str.contains('California') & all_datasets['TableType'].str.contains('STOPS')),\n",
    "                            all_datasets=all_datasets,\n",
    "                            debug_filename=\"removed_california_stops_datasets.csv\")\n",
    "\n",
    "\n",
    "# Remove datasets where the year is not specified\n",
    "all_datasets=remove_rows(remove_condition=all_datasets['Year']== 'NONE',\n",
    "                        all_datasets=all_datasets,\n",
    "                        debug_filename=\"removed_year_none_datasets.csv\")\n",
    "\n",
    "\n",
    "# Remove datasets with empty coverage dates. This should be redundant with the Year being NONE\n",
    "all_datasets = remove_rows(remove_condition=all_datasets[['coverage_start', 'coverage_end']].isnull().any(axis=1),\n",
    "                           all_datasets=all_datasets,\n",
    "                           debug_filename=\"empty_coverage.csv\")\n",
    "\n",
    "                  \n",
    "# focus on the main type for example, change\n",
    "# \"USE OF FORCE - INCIDENTS\" and\n",
    "# \"USE OF FORCE - SUBJECTS/OFFICERS\"\n",
    "# to just \"USE OF FORCE\"\n",
    "all_datasets['TableType'] = all_datasets['TableType'].str.split(' - ').str[0]\n",
    "\n",
    "# Combine stops and traffic stops datasets. Also include pedestrian stops.\n",
    "all_datasets['TableType'] = all_datasets['TableType'].apply(lambda x: 'STOPS' if 'STOPS' in x else x)\n",
    "\n",
    "print(f\"Excluded a total of {all_datasets_original.shape[0] - all_datasets.shape[0]} datasets\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we will organize the data to make it easier to analyze\n",
    "# Since we are interested in types of datasets that have been stopped, we want to focus on coverage start and end dates and table types\n",
    "selected_columns = ['State', 'SourceName', 'AgencyFull', 'TableType', 'coverage_start', 'coverage_end']\n",
    "df = all_datasets.copy()\n",
    "\n",
    "# Create entries for each year instead of a range of years. This will make it easier to filter the data\n",
    "df['ListOfYears'] = df.apply(\n",
    "    lambda row: list(range(int(row['coverage_start'].year), int(row['coverage_end'].year) + 1)), axis=1)\n",
    "\n",
    "# no need for the coverage_start and coverage_end columns since we have the list of years\n",
    "df = df.drop(columns=['coverage_start', 'coverage_end'])\n",
    "\n",
    "# find any columns of 'ListOfYears' that are not list objects and print those rows\n",
    "# There shouldn't be any. This is just a data quality check.\n",
    "non_list_years = df[~df['ListOfYears'].apply(lambda x: isinstance(x, list))]\n",
    "\n",
    "if len(non_list_years) > 0:\n",
    "    raise ValueError(f\"Error, the ListOfYears column has {len(non_list_years)} values that are not lists are:\\n{non_list_years}\")\n",
    "else:\n",
    "    pass # do nothing this is just a quality check, everything should be a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Focus on organizing up the ListOfYears column\n",
    "\n",
    "# merge rows that have the same values by extending the 'ListOfYears' column values into a single list\n",
    "# Specifically this will merge table types that had \" - \" such as \"USE OF FORCE - INCIDENTS\" and \"USE OF FORCE - SUBJECTS/OFFICERS\"\n",
    "df = df.groupby(['State', 'SourceName', 'TableType', 'AgencyFull']).agg({'ListOfYears': 'sum'}).reset_index()\n",
    "# remove any duplicate years from the 'ListOfYears' column and sort the list\n",
    "df['ListOfYears'] = df['ListOfYears'].apply(lambda x: sorted(list(set(x))))\n",
    "\n",
    "# Data quality check: Ensure that there are no duplicated tables\n",
    "duplicates = df[df.duplicated(['State', 'SourceName', 'TableType'], keep=False)]\n",
    "if len(duplicates) > 0:\n",
    "    print(f\"Error, the dataset has {len(duplicates)} duplicate rows. They are:\")\n",
    "    print(duplicates)\n",
    "else:\n",
    "    pass # do nothing since just another quality check\n",
    "\n",
    "\n",
    "# Calculate the minimum and maximum years for each dataset\n",
    "df['MinYear'] = df['ListOfYears'].apply(min)\n",
    "df['MaxYear'] = df['ListOfYears'].apply(max)\n",
    "\n",
    "# Create a label for plotting\n",
    "df['Label'] = df['SourceName'] + ', ' + df['State'] + ', ' + df['TableType']\n",
    "\n",
    "# Data quality check: find any rows where the MinYear is greater than the MaxYear or the MinYear is less than analysis_year_min or either the MinYear or MaxYear are not integers\n",
    "invalid_years = df[(df['MinYear'] > df['MaxYear']) | (df['MinYear'] < analysis_year_min) | (df['MinYear'] % 1 != 0) | (df['MaxYear'] % 1 != 0)]\n",
    "\n",
    "# if invalid years remove them and print a warning about it\n",
    "if len(invalid_years) > 0:\n",
    "    print(f\"Warning, the dataset has {len(invalid_years)} rows with invalid years. They are:\\n{invalid_years}\")\n",
    "    df = remove_rows(remove_condition=df.index.isin(invalid_years.index),\n",
    "                     all_datasets=df,\n",
    "                     debug_filename=\"removed_invalid_years.csv\")\n",
    "else:\n",
    "    pass\n",
    "#    raise ValueError(f\"Error, the dataset has {len(invalid_years)} rows with invalid years. They are:\\n{invalid_years}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if clean_ois:\n",
    "    # Remove officer-involved shooting (OIS) datasets where there is no recent data and the Mapping Police Violence \n",
    "    # database agrees that there have been no recent OIS. This helps account for the potential rarity of OIS.\n",
    "    num_before = len(df)\n",
    "    df = validate_ois_datasets(df)\n",
    "    print(f'Number of officer-involved shootings datasets removed: {num_before-len(df)}\\n')\n",
    "\n",
    "print(f'Number of datasets that will be analyzed: {len(df)}')\n",
    "df.to_csv('stopped_dataset_analysis_datasets.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_df= df.copy()\n",
    "\n",
    "# Increase min year to shrink x-limits\n",
    "min_year = 2010\n",
    "plot_df['MinYear'] = plot_df['MinYear'].apply(lambda x: min_year if x<min_year else x)\n",
    "\n",
    "# If the MinYear is the same as the MaxYear, subtract 1 from MinYear to make it viewable \n",
    "plot_df['MinYear'] = plot_df.apply(lambda row: row['MinYear'] - 1 if row['MinYear'] == row['MaxYear'] else row['MinYear'], axis=1)\n",
    "\n",
    "plot_df = plot_df.sort_values(by=['TableType','MaxYear'])\n",
    "\n",
    "# Plot the data to see the years of data available for each unique combination of 'SourceName', 'State', 'TableType'\n",
    "plt.figure(figsize=(7, 60))\n",
    "ax = plt.barh(plot_df['Label'], plot_df['MaxYear'] - plot_df['MinYear'], left=plot_df['MinYear'], color='blue')\n",
    "plt.xlabel('Year')\n",
    "plt.title('Years of Data Available by Source, State, and Table Type')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find datasets whose most recent year is up_to_date_dataset_year_max or earlier\n",
    "\n",
    "\n",
    "tabletype_analysis_df = df.copy()\n",
    "tabletype_counts = tabletype_analysis_df['TableType'].value_counts()\n",
    "\n",
    "# show only statistically significant table types\n",
    "total_tabletype_counts = len(tabletype_counts)\n",
    "tabletype_counts = tabletype_counts[tabletype_counts >= minimum_tabletype_counts_to_show]\n",
    "print(f\"Table type counts >= {minimum_tabletype_counts_to_show} length is {len(tabletype_counts)}. The total number of table types is {total_tabletype_counts}\")\n",
    "\n",
    "# filter out the table types that are not in the tabletype_counts\n",
    "tabletype_analysis_df = tabletype_analysis_df[tabletype_analysis_df['TableType'].isin(tabletype_counts.index)]\n",
    "\n",
    "print(f'Number of datasets from the {len(tabletype_counts)} tables is: {len(tabletype_analysis_df)}')\n",
    "\n",
    "stopped_datasets = tabletype_analysis_df[tabletype_analysis_df['MaxYear'] < up_to_date_dataset_year_max]\n",
    "stopped_tabletype_counts = stopped_datasets['TableType'].value_counts()\n",
    "\n",
    "\n",
    "# compute a bar graph histogram of the number of datasets that are stopped by TableType\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(14, 6))\n",
    "\n",
    "tabletype_counts.plot(kind='bar', ax=axes[0])\n",
    "axes[0].set_xlabel('Table Type')\n",
    "axes[0].set_ylabel('Number of Datasets')\n",
    "axes[0].set_title('Number of Datasets by Table Type')\n",
    "axes[0].set_xticklabels(tabletype_counts.index, rotation=45, ha='right')\n",
    "\n",
    "stopped_tabletype_counts.plot(kind='bar', ax=axes[1])\n",
    "axes[1].set_xlabel('Table Type')\n",
    "axes[1].set_ylabel('Number of Datasets')\n",
    "axes[1].set_title('Number of Stopped Datasets by Table Type')\n",
    "axes[1].set_xticklabels(stopped_tabletype_counts.index, rotation=45, ha='right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Find which type has the highest ratio of stopped datasets\n",
    "percentage_of_stopped_datasets = 100*(stopped_tabletype_counts / tabletype_counts)\n",
    "percentage_of_stopped_datasets = percentage_of_stopped_datasets.fillna(0)\n",
    "\n",
    "# Create a bar plot of the ratio and sort the values from high to low\n",
    "percentage_of_stopped_datasets = percentage_of_stopped_datasets.sort_values(ascending=False)\n",
    "plt.figure(figsize=(10, 6))\n",
    "ax = percentage_of_stopped_datasets.plot(kind='bar')\n",
    "ax.set_xticklabels(percentage_of_stopped_datasets.index, rotation=45, ha='right')\n",
    "plt.xlabel('Table Type')\n",
    "plt.ylabel('Percentage of Stopped Datasets')\n",
    "plt.title('Percentage of Stopped Datasets by Table Type')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the table type with highest stop percentage from previous analysis\n",
    "table_type = None #'OFFICER-INVOLVED SHOOTINGS'  # Enter None for all datasets or a table_type for specific table type\n",
    "if table_type:\n",
    "    show_datasets = stopped_datasets[stopped_datasets['TableType']==table_type]\n",
    "else:  # Set to None to show all\n",
    "    show_datasets = stopped_datasets\n",
    "year_counts = show_datasets['MaxYear'].value_counts().sort_index()\n",
    "year_counts.plot.bar(ylabel=\"# of Stopped Datasets\", xlabel='Last Year of Data')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.title(f\"Stopped Datasets by Year for {table_type if table_type else 'All Datasets'}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze trends in stopped datasets by year and table type\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Create a DataFrame to track when datasets stopped\n",
    "trend_df = stopped_datasets.copy()\n",
    "\n",
    "# Group by TableType and MaxYear (the year when the dataset stopped being updated)\n",
    "yearly_stops = trend_df.groupby(['TableType', 'MaxYear']).size().unstack(fill_value=0)\n",
    "\n",
    "# Plot yearly (non-cumulative) stopped datasets\n",
    "plt.figure(figsize=(12, 6))\n",
    "ax = plt.axes()\n",
    "yearly_stops.T.plot.bar(stacked=True, ax=ax)\n",
    "\n",
    "# for table_type in yearly_stops.index:\n",
    "#     # if table_type in tabletype_counts.nlargest(6).index:\n",
    "#     plt.plot(yearly_stops.loc[table_type].index, \n",
    "#                 yearly_stops.loc[table_type].values, \n",
    "#                 marker='o', \n",
    "#                 linewidth=2, \n",
    "#                 label=table_type)\n",
    "\n",
    "plt.title('Number of Datasets Stopped Each Year by Table Type')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Count of Datasets Stopped')\n",
    "plt.legend(loc='upper left')\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate cumulative counts for each table type over time\n",
    "cumulative_stops = yearly_stops.cumsum(axis=1)\n",
    "\n",
    "# Select the top table types for clearer visualization\n",
    "#top_types = tabletype_counts.nlargest(6).index.tolist()\n",
    "top_types = tabletype_counts.index.tolist()\n",
    "cumulative_filtered = cumulative_stops.loc[cumulative_stops.index.isin(top_types)]\n",
    "\n",
    "# Plot the cumulative trends\n",
    "plt.figure(figsize=(12, 6))\n",
    "ax = plt.axes()\n",
    "cumulative_filtered.T.plot.bar(stacked=True, ax=ax)\n",
    "# for table_type in cumulative_filtered.index:\n",
    "#     plt.plot(cumulative_filtered.loc[table_type].index, \n",
    "#              cumulative_filtered.loc[table_type].values, \n",
    "#              marker='o', \n",
    "#              linewidth=2, \n",
    "#              label=table_type)\n",
    "\n",
    "plt.title('Cumulative Number of Stopped Datasets by Year and Table Type')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Cumulative Count of Stopped Datasets')\n",
    "plt.legend(loc='upper left')\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Create a normalized version to compare relative changes\n",
    "plt.figure(figsize=(12, 6))\n",
    "for table_type in cumulative_filtered.index:\n",
    "    # Normalize by the total count for each type\n",
    "    total = tabletype_counts[table_type]\n",
    "    plt.plot(cumulative_filtered.loc[table_type].index, \n",
    "             100 * cumulative_filtered.loc[table_type].values / total, \n",
    "             marker='o', \n",
    "             linewidth=2, \n",
    "             label=f\"{table_type} (total: {total})\")\n",
    "\n",
    "plt.title('Percentage of Datasets Stopped by Year and Table Type')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Percentage of Datasets Stopped (%)')\n",
    "plt.legend(loc='upper left')\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regional analysis for a specific table type\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "table_type = None # 'STOPS' # # Enter None for all datasets or a table_type for specific table type\n",
    "\n",
    "print(f\"Analyzing regional patterns for: {table_type if table_type else 'All datasets'}\")\n",
    "\n",
    "# Filter datasets for the selected table type\n",
    "if table_type:\n",
    "    type_datasets = df[df['TableType'] == table_type].copy()\n",
    "else:\n",
    "    type_datasets = df.copy()\n",
    "\n",
    "# Group by region (state) and calculate stopped percentage\n",
    "region_analysis = type_datasets.groupby('State').apply(\n",
    "    lambda x: pd.Series({\n",
    "        'total_count': len(x),\n",
    "        'stopped_count': sum(x['MaxYear'] < up_to_date_dataset_year_max),\n",
    "        'stopped_percent': 100 * sum(x['MaxYear'] < up_to_date_dataset_year_max) / len(x)\n",
    "    })\n",
    ").reset_index()\n",
    "\n",
    "# Sort by percentage for better visualization\n",
    "region_analysis = region_analysis.sort_values('stopped_percent', ascending=False)\n",
    "\n",
    "# Only include states with at least 2 datasets of this type\n",
    "min_datasets = 2\n",
    "filtered_regions = region_analysis[region_analysis['total_count'] >= min_datasets]\n",
    "\n",
    "# Plot regional analysis\n",
    "plt.figure(figsize=(14, 8))\n",
    "bars = plt.bar(filtered_regions['State'], filtered_regions['stopped_percent'], \n",
    "        color='steelblue', alpha=0.8)\n",
    "\n",
    "# Add count labels on top of bars\n",
    "for bar, total, stopped in zip(bars, filtered_regions['total_count'], filtered_regions['stopped_count']):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, 5, \n",
    "             f\"{stopped}/{total}\", \n",
    "             ha='center', va='bottom', \n",
    "             fontweight='bold', color='black', rotation=90)\n",
    "\n",
    "plt.title(f'Percentage of Stopped {table_type+' ' if table_type else ''}Datasets by State (min {min_datasets} datasets)')\n",
    "plt.xlabel('State')\n",
    "plt.ylabel('Percentage Stopped (%)')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.ylim(0, 105)  # Leave room for labels\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Create a heatmap of stop years by region for this table type\n",
    "stop_year_matrix = pd.pivot_table(\n",
    "    type_datasets[type_datasets['MaxYear'] < up_to_date_dataset_year_max],\n",
    "    index='State', \n",
    "    columns='MaxYear', \n",
    "    values='SourceName',\n",
    "    aggfunc='count',\n",
    "    fill_value=0\n",
    ")\n",
    "\n",
    "filtered_regions['up-to-date_count'] = filtered_regions['total_count'] - filtered_regions['stopped_count']\n",
    "\n",
    "plt.figure(figsize=(14, 8))\n",
    "ax = plt.axes()\n",
    "filtered_regions.set_index('State').drop(columns=['stopped_percent', 'total_count']).plot.bar(stacked=True, ax=ax)\n",
    "\n",
    "plt.title(f'Percentage of Stopped {table_type+' ' if table_type else ''}Datasets by State (min {min_datasets} datasets)')\n",
    "plt.xlabel('State')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "# plt.ylim(0, 105)  # Leave room for labels\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_3_11_9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
